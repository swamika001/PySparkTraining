{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 3: Dataframes\n",
    "\n",
    "## 1. Analyzing Transactions\n",
    "\n",
    "*This problem is taken from [Data Algorithms: Recipes for Scaling Up with Hadoop and Spark](http://shop.oreilly.com/product/0636920033950.do) by Mahmoud Parsian.*\n",
    "\n",
    "Consider a company such as Amazon, which has over 200 million users and can do hundreds of millions of transactions per day. The user data consists of users’ location information and the transaction data includes user identity information, but no direct information about a user’s location. We are interested in finding out which countries are our best consumers in terms of average order total cost.\n",
    "\n",
    "### 1.1 Dataset\n",
    "We have access to seven tables of a store's database as tab separated value files (`.tsv`):\n",
    "- customers (`data/store/customers.tsv`)\n",
    "- orders (`data/store/orders.tsv`)\n",
    "- order details (`data/store/order_details.tsv`)\n",
    "- products (`data/store/products.tsv`)\n",
    "- categories (`data/store/categories.tsv`)\n",
    "- shippers (`data/store/shippers.tsv`)\n",
    "- suppliers (`data/store/suppliers.tsv`)\n",
    "\n",
    "The tables can be viewed online: [SQL Tutorial Database](https://docs.google.com/spreadsheets/d/1_rn2PWl5qqw7ZuBuCm6D7FJrHepOlunjpCPbvitdxxQ/edit?usp=sharing).\n",
    "\n",
    "### 1.2 Objective\n",
    "We are interested in finding the average order cost per country.\n",
    "\n",
    "### 1.3 Instructions\n",
    "\n",
    "1. Identify the fields that you will need to find the answer; \n",
    "2. Initialize the Spark and Spark SQL contexts;\n",
    "3. Read the datasets you need as dataframes;\n",
    "4. Join the tables and keep only the fields you need;\n",
    "5. Compute the total cost per order;\n",
    "6. Compute the average order cost per country;\n",
    "7. Display the countries that are spending the most.\n",
    "\n",
    "### 1.4 Hints\n",
    "\n",
    "- If you go through the dataset, you will see that the information required to fulfill the objective is scattered among the different tables.\n",
    "- An order may include multiple products, you will need to aggregate the order details to compute the order total cost.\n",
    "- Some of the methods required in this practice have not been covered so far, you will need to look at the [DataFrame's API](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n",
    "- If you are stuck, take a peek at the [Recap](#2.-Recap) section to get insights on what functions you should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    sc = pyspark.SparkContext()\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customers = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                      .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "                      .load('data/store/customers.tsv')\n",
    "\n",
    "details = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                    .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "                    .load('data/store/order_details.tsv')\n",
    "\n",
    "orders = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                   .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "                   .load('data/store/orders.tsv')\n",
    "\n",
    "products = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                     .options(header='true', inferschema='true', delimiter='\\t')\\\n",
    "                     .load('data/store/products.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify what fields you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_orders = orders.select('OrderID', 'CustomerID')\n",
    "sub_clients = customers.select('CustomerID', 'Country')\n",
    "sub_details = details.select('OrderID', 'ProductID', 'Quantity')\n",
    "sub_products = products.select('ProductID', 'Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sub_orders.join(sub_clients, 'CustomerID')\n",
    "df = df.join(sub_details, 'OrderID')\n",
    "df = df.join(sub_products, 'ProductID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('Total', df.Quantity * df.Price)\\\n",
    "        .select('OrderID', 'Country', 'Total')\\\n",
    "        .groupBy('OrderID', 'Country')\\\n",
    "        .agg({'Total' : 'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.groupBy('Country').agg({'sum(Total)' : 'avg'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the 5 most spending countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Country|   avg(sum(Total))|\n",
      "+-------+------------------+\n",
      "|Denmark|         4467.7125|\n",
      "|Belgium|           4025.65|\n",
      "|Austria| 3974.766153846154|\n",
      "| Canada|3480.7055555555557|\n",
      "|Ireland|           2565.17|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.orderBy('avg(sum(Total))', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark SQL API](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html)**:\n",
    "1. Import Spark SQL Python module: \n",
    "**[`import pyspark.sql`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)**\n",
    "2. Create a Spark SQLContext:\n",
    "**[`pyspark.sql.SQLContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext)**\n",
    "1. Create an RDD from a CSV file:\n",
    "**[`SQLContext.read.format('com.databricks.spark.csv')`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.SQLContext.read)**\n",
    "2. Select a subset of variables from a DataFrame : **[`DataFrame.select(*cols)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select)**\n",
    "3. Join two DataFrames : **[`DataFrame.join(DataFrame)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join)**\n",
    "4. Create a new DataFrame by adding a column to an existing one: **[`DataFrame.withColumn(colName, col)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn)**\n",
    "5. Group the DataFrame using the specified columns: **[`DataFrame.groupBy(*cols)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy)**\n",
    "6. Compute aggregates on GroupeData and return a new DataFrame: **[`GroupedData.agg(*exprs)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg)**\n",
    "7. Sort a DataFrame by the specified columns: **[`DataFrame.orderBy(*cols)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy)**\n",
    "8. Print the first `n` rows of a DataFrame to the console: **[`DataFrame.show(n=20)`](https://spark.apache.org/docs/1.6.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Data Algorithms: Recipes for Scaling Up with Hadoop and Spark](http://shop.oreilly.com/product/0636920033950.do) by Mahmoud Parsian\n",
    "* [W3C School - Store Dataset](http://www.w3schools.com/sql/trysql.asp?filename=trysql_select_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
