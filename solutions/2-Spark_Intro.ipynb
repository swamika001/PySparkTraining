{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Interactive Analysis with Spark\n",
    "\n",
    "## Table of Content\n",
    "1. [Initialization](#1.-Initialization)\n",
    "2. [Creating an RDD](#2.-Creating-an-RDD)\n",
    "3. [Getting Help](#3.-Getting-Help)\n",
    "4. [Action on a Dataset](#4.-Action-on-a-Dataset)\n",
    "5. [Dataset Transformation](#5.-Dataset-Transformation)\n",
    "6. [Caching a Dataset](#6.-Caching-a-Dataset)\n",
    "7. [Filtering a Dataset](#7.-Filtering-a-Dataset)\n",
    "8. [Reduction Operation](#8.-Reduction-Operation)  \n",
    "  8.1 [Transforming in Key-Value Pairs](#8.1-Transforming-in-Key-Value-Pairs)  \n",
    "  8.2 [Filtering Unrelated Entries](#8.2-Filtering-Unrelated-Entries)  \n",
    "  8.3 [Aggregating the Results by Key](#8.3-Aggregating-the-Results-by-Key)  \n",
    "  8.4 [Computing the Most Frequent Languages](#8.4-Computing-the-Most-Frequent-Languages)    \n",
    "  8.5 [Bar Chart](#8.5.-Bar-Chart)\n",
    "9. [Ending the Analysis](#9.-Ending-the-Analysis)\n",
    "10. [Recap](#10.-Recap)\n",
    "11. [References](#11.-References)\n",
    "\n",
    "## List of Exercises\n",
    "1. [Exercise 1: How to Cache?](#Exercise-1)\n",
    "2. [Exercise 2: How to Peek?](#Exercise-2)\n",
    "3. [Exercise 3: How to Filter?](#Exercise-3)\n",
    "4. [Exercise 4: How to Sort?](#Exercise-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization\n",
    "\n",
    "In this notebook, we will use Spark to analyze brievly a simple structured dataset.\n",
    "\n",
    "First, we need to import Spark's Python module named `pyspark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import spark_helper # homemade module to ease the workshop progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : a SparkContext already exists.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc = pyspark.SparkContext()\n",
    "except ValueError:\n",
    "    print(\"Warning : a SparkContext already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we execute Spark locally, the context creation can be used to Spark. It is also possible to launch Spark and the Python interpreter / notebook using the script named `pyspark`. In this case, the context will already be created (under the name `sc`) and a `ValueError` exception is raised when we try to create a second context. The exception has not effect so we simply catch it and display a warning message.\n",
    "\n",
    "We can consult the dashboard of our Spark application at the link computed by the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark app's dashboard link: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark app's dashboard link: {}\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an RDD from text files representing the page visits data of Wikipedia. This data should be in the folder `data/pagecounts`. \n",
    "\n",
    "The notebook [0-Configuration.ipynb](0-Configuration.ipynb) can assist you in downloading the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecounts = sc.textFile('data/pagecounts/*.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [pagecounts](https://dumps.wikimedia.org/other/pagecounts-raw/) file looks like this  \n",
    "```\n",
    "af Spesiaal:Onlangse_wysigings 3 101681\n",
    "af Spesiaal:RecentChanges 2 2248\n",
    "af Suid-Afrika 1 30698\n",
    "af Tuisblad 14 155257 \n",
    "af Varkgriep 4 42236\n",
    "af Wikipedia 2 32796\n",
    "```\n",
    "\n",
    "It is a tabular file, where each line is a distinct entry and the columns represent\n",
    "2. the project name (language);\n",
    "3. the page title;\n",
    "4. the number of requests;\n",
    "5. the size of the content returned.\n",
    "\n",
    "We can look at a few entries with the RDD's method `take` to get the first `K` elements of the dataset. Here, `K = 10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa.b Main_Page 3 16288', 'aa.b Special:RecentChangesLinked/User:Az1568 1 5745', 'aa.b Special:Recentchangeslinked/User:Az1568 1 1013', 'aa.b Special:Statistics 1 840', 'aa.b Template:Delete 1 26601', 'aa.d Main_Page 2 5442', 'aa Main_Page 4 19955', 'aa User:Alecs.bot 1 13388', 'ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465', 'ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16098']\n"
     ]
    }
   ],
   "source": [
    "first10 = pagecounts.take(10)\n",
    "print(first10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since `take` returns a list, we can iterate on the result and print it \"prettily\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa.b Main_Page 3 16288\n",
      "aa.b Special:RecentChangesLinked/User:Az1568 1 5745\n",
      "aa.b Special:Recentchangeslinked/User:Az1568 1 1013\n",
      "aa.b Special:Statistics 1 840\n",
      "aa.b Template:Delete 1 26601\n",
      "aa.d Main_Page 2 5442\n",
      "aa Main_Page 4 19955\n",
      "aa User:Alecs.bot 1 13388\n",
      "ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465\n",
      "ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16098\n"
     ]
    }
   ],
   "source": [
    "for item in first10:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting Help\n",
    "\n",
    "At any moment, you can get help on a Python object using the `help()` function. For example, if we want to know more aboud the RDD's `take()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method take in module pyspark.rdd:\n",
      "\n",
      "take(num) method of pyspark.rdd.RDD instance\n",
      "    Take the first num elements of the RDD.\n",
      "    \n",
      "    It works by first scanning one partition, and use the results from\n",
      "    that partition to estimate the number of additional partitions needed\n",
      "    to satisfy the limit.\n",
      "    \n",
      "    Translated from the Scala implementation in RDD#take().\n",
      "    \n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      "    [2, 3]\n",
      "    >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      "    [2, 3, 4, 5, 6]\n",
      "    >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      "    [91, 92, 93]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pagecounts.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Action on a Dataset\n",
    "\n",
    "The `take()` method is one among multiple available *actions* we apply on an RDD. An exhaustive list of action is available at the following URL:\n",
    "https://spark.apache.org/docs/latest/programming-guide.html#actions\n",
    "\n",
    "In case where we do not want to leave the notebook tab, we can call `help()` directly on an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n",
      "\n",
      "class RDD(builtins.object)\n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  aggregate(self, zeroValue, seqOp, combOp)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persist this RDD with the default storage level (C{MEMORY_ONLY_SER}).\n",
      " |  \n",
      " |  cartesian(self, other)\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
      " |      C{b} is in C{other}.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self)\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |  \n",
      " |  coalesce(self, numPartitions, shuffle=False)\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self, other, numPartitions=None)\n",
      " |      For each key k in C{self} or C{other}, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in C{self} as\n",
      " |      well as C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Return a list that contains all of the elements in this RDD.\n",
      " |  \n",
      " |  collectAsMap(self)\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  combineByKey(self, createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.  Note that V and C can be different -- for example, one might\n",
      " |      group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      Users provide three functions:\n",
      " |      \n",
      " |          - C{createCombiner}, which turns a V into a C (e.g., creates\n",
      " |            a one-element list)\n",
      " |          - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
      " |            a list)\n",
      " |          - C{mergeCombiners}, to combine two C's into a single one.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> def add(a, b): return a + str(b)\n",
      " |      >>> sorted(x.combineByKey(str, add, add).collect())\n",
      " |      [('a', '11'), ('b', '1')]\n",
      " |  \n",
      " |  count(self)\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self, relativeSD=0.05)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      \"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available\n",
      " |      <a href=\"http://dx.doi.org/10.1145/2452376.2452456\">here</a>.\n",
      " |      \n",
      " |      :param relativeSD: Relative accuracy. Smaller values create\n",
      " |                         counters that require more space.\n",
      " |                         It must be greater than 0.000017.\n",
      " |      \n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self)\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self)\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self, numPartitions=None)\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self, f)\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self)\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      >>> x.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self, zeroValue, op)\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative and commutative function and\n",
      " |      a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify C{t2}.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      >>> def f(x): print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> def f(iterator):\n",
      " |      ...      for x in iterator:\n",
      " |      ...           print(x)\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
      " |      (k, (None, w)) if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self)\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |  \n",
      " |  getNumPartitions(self)\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getStorageLevel(self)\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1,2])\n",
      " |      >>> rdd1.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd1.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self)\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self, f, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      Note: If you are grouping in order to perform an aggregation (such as a\n",
      " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |      provide much better performance.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self, other, *others)\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self, buckets)\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) inseration to O(1) per\n",
      " |      element(where n = # buckets).\n",
      " |      \n",
      " |      Buckets must be sorted and not contain any duplicates, must be\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generates buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given buckets\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. buckets must\n",
      " |      be at least 1 If the RDD contains infinity, NaN throws an exception\n",
      " |      If the elements in RDD do not vary (max == min) always returns\n",
      " |      a single bucket.\n",
      " |      \n",
      " |      It will return an tuple of buckets and histogram.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self)\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |  \n",
      " |  intersection(self, other)\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      Note that this method performs a shuffle internally.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self)\n",
      " |      Return whether this RDD has been checkpointed or not\n",
      " |  \n",
      " |  isEmpty(self)\n",
      " |      Returns true if and only if the RDD contains no elements at all. Note that an RDD\n",
      " |      may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  join(self, other, numPartitions=None)\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      C{self} and C{other}.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(x.join(y).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self, f)\n",
      " |      Creates tuples of the elements in this RDD by applying C{f}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self)\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> m.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a left outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, v) in C{self}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
      " |      (k, (v, None)) if no elements in C{other} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  lookup(self, key)\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self, f, preservesPartitioning=False)\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self, f, preservesPartitioning=False)\n",
      " |      Deprecated: use mapPartitionsWithIndex instead.\n",
      " |      \n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self, f)\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      >>> x.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self, key=None)\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self)\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self, key=None)\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      :param key: A function used to generate key for comparing\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self)\n",
      " |      Return the name of this RDD.\n",
      " |  \n",
      " |  partitionBy(self, numPartitions, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(False, True, False, False, 1))\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_ONLY_SER}).\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |  \n",
      " |  pipe(self, command, env=None, checkCode=False)\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |      \n",
      " |      :param checkCode: whether or not to check the return value of the shell command.\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      :param weights: weights for splits, will be normalized if they don't sum to 1\n",
      " |      :param seed: random seed\n",
      " |      :return: split RDDs in a list\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self, f)\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self, func, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>)\n",
      " |      Merge the values for each key using an associative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with C{numPartitions} partitions, or\n",
      " |      the default parallelism level if C{numPartitions} is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self, func)\n",
      " |      Merge the values for each key using an associative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self, numPartitions)\n",
      " |      Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |      Can increase or decrease the level of parallelism in this RDD.\n",
      " |      Internally, this uses a shuffle to redistribute data.\n",
      " |      If you are decreasing the number of partitions in this RDD, consider\n",
      " |      using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |      >>> len(rdd.repartition(2).glom().collect())\n",
      " |      2\n",
      " |      >>> len(rdd.repartition(10).glom().collect())\n",
      " |      10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=<function portable_hash at 0x7f34a2da4d90>, ascending=True, keyfunc=<function RDD.<lambda> at 0x7f34a802dd08>)\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self, other, numPartitions=None)\n",
      " |      Perform a right outer join of C{self} and C{other}.\n",
      " |      \n",
      " |      For each element (k, w) in C{other}, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in C{self} have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self, withReplacement, fraction, seed=None)\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
      " |      :param fraction: expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      :param seed: seed for the random number generator\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self, withReplacement, fractions, seed=None)\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self)\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self)\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: (None by default)\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
      " |      \n",
      " |      :param conf: Hadoop job configuration, passed in as a dict\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
      " |      C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      :param path: path to Hadoop file\n",
      " |      :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
      " |             (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      :param keyClass: fully qualified classname of key Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      :param valueClass: fully qualified classname of value Writable class\n",
      " |             (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      :param keyConverter: (None by default)\n",
      " |      :param valueConverter: (None by default)\n",
      " |      :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
      " |  \n",
      " |  saveAsPickleFile(self, path, batchSize=10)\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tmpFile.close()\n",
      " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
      " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
      " |      ['1', '2', 'rdd', 'spark']\n",
      " |  \n",
      " |  saveAsSequenceFile(self, path, compressionCodecClass=None)\n",
      " |      Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
      " |      system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      :param path: path to sequence file\n",
      " |      :param compressionCodecClass: (None by default)\n",
      " |  \n",
      " |  saveAsTextFile(self, path, compressionCodecClass=None)\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      @param path: path to text file\n",
      " |      @param compressionCodecClass: (None by default) string i.e.\n",
      " |          \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      \n",
      " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile.close()\n",
      " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile2.close()\n",
      " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
      " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
      " |      >>> tempFile3.close()\n",
      " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      >>> b''.join(result).decode('utf-8')\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self, name)\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      >>> rdd1 = sc.parallelize([1, 2])\n",
      " |      >>> rdd1.setName('RDD1').name()\n",
      " |      'RDD1'\n",
      " |  \n",
      " |  sortBy(self, keyfunc, ascending=True, numPartitions=None)\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self, ascending=True, numPartitions=None, keyfunc=<function RDD.<lambda> at 0x7f34a802de18>)\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      # noqa\n",
      " |      \n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self)\n",
      " |      Return a L{StatCounter} object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |  \n",
      " |  stdev(self)\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self, other, numPartitions=None)\n",
      " |      Return each value in C{self} that is not contained in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtract(y).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self, other, numPartitions=None)\n",
      " |      Return each (key, value) pair in C{self} that has no pair with matching\n",
      " |      key in C{other}.\n",
      " |      \n",
      " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(x.subtractByKey(y).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self)\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self, timeout, confidence=0.95)\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self, num, key=None)\n",
      " |      Get the N elements from a RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |  \n",
      " |  takeSample(self, withReplacement, num, seed=None)\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |  \n",
      " |  toDF(self, schema=None, sampleRatio=None)\n",
      " |      Converts current :class:`RDD` into a :class:`DataFrame`\n",
      " |      \n",
      " |      This is a shorthand for ``sqlContext.createDataFrame(rdd, schema, sampleRatio)``\n",
      " |      \n",
      " |      :param schema: a StructType or list of names of columns\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      >>> rdd.toDF().collect()\n",
      " |      [Row(name=u'Alice', age=1)]\n",
      " |  \n",
      " |  toDebugString(self)\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self, num, key=None)\n",
      " |      Get the top N elements from a RDD.\n",
      " |      \n",
      " |      Note: It returns the list sorted in descending order.\n",
      " |      \n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self, zeroValue, seqOp, combOp, depth=2)\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self, f, depth=2)\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      :param depth: suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self)\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |  \n",
      " |  values(self)\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> m.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self)\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  zip(self, other)\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      >>> x = sc.parallelize(range(0,5))\n",
      " |      >>> y = sc.parallelize(range(1000, 1005))\n",
      " |      >>> x.zip(y).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self)\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self)\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      L{zipWithIndex}\n",
      " |      \n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  context\n",
      " |      The L{SparkContext} that this RDD was created on.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pagecounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the available action, lets take the method `count()`. What does it return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11268665"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each action apply on an RDD leads to the creation of one or many task and the production of a result. Every task executed in the same app can be visualised in the Spark's dashboard. In this interface, we can track the progress of a task, and check different performance measures on the task, for example its duration and cache statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the 10 first element of our dataset that we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa.b Main_Page 3 16288',\n",
       " 'aa.b Special:RecentChangesLinked/User:Az1568 1 5745',\n",
       " 'aa.b Special:Recentchangeslinked/User:Az1568 1 1013',\n",
       " 'aa.b Special:Statistics 1 840',\n",
       " 'aa.b Template:Delete 1 26601',\n",
       " 'aa.d Main_Page 2 5442',\n",
       " 'aa Main_Page 4 19955',\n",
       " 'aa User:Alecs.bot 1 13388',\n",
       " 'ab %D0%90%D0%B8%D0%BD%D1%82%D0%B5%D1%80%D0%BD%D0%B5%D1%82 1 465',\n",
       " 'ab %D0%98%D1%85%D0%B0%D0%B4%D0%BE%D1%83_%D0%B0%D0%B4%D0%B0%D2%9F%D1%8C%D0%B0 1 16098']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that the RDD is composed of each line of input text files, but that is not possible to access to individual column. **Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aa.b Main_Page 3 16288'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first1 = pagecounts.first()\n",
    "first1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action `first()` as its name states, return the first entry of the dataset. We see that each entry is a single string. We will need to transform that first RDD in a second in order to divide each string in a list of 4 elements. To do this, we will use the method `split()` of Python string object.\n",
    "\n",
    "We first test it on the dataset first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa.b', 'Main_Page', '3', '16288']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(first1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to apply this transformation to every RDD's entry. The RDD's method `map(func)` returns a new distributed dataset formed by passing each element of the source through a function *func*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecounts_tab = pagecounts.map(str.split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of this transformation is *lazy*. Spark does not compute anything as long as a result is not requested by an action. To convince yourself, execute the preceding cell, then visit the Spark dashboard. You should see that no job have been added to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark app's dashboard link: http://localhost:4040/jobs\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark app's dashboard link: {}/jobs\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Caching a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we expect to operate frequently on the same dataset, it can be useful to tell Spark to keep it in memory.\n",
    "\n",
    "To do so, we use the `cache()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_tab.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDDs stored in memory are displayed in the **Storage** section of Spark web interface. Note that datasets are not loaded in memory until an action is called on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark app's dashboard link: http://localhost:4040/storage\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark app's dashboard link: {}/storage\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To free memory used by cached RDD that we no longer need, we need to call the `unpersist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[5] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_tab.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Filtering a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have an RDD that is easier to manipulate, we can star the analysis. First, we take interest in page s rom Wikipedia English. The following line filter the last RDD we created , eepsing nly the entries written in English.\n",
    "\n",
    "Try to answer the following quiz before executing the cell:  \n",
    "* What sort of argument takes the `filter()` method?\n",
    "* What type is `x`?\n",
    "* Is filter an action or a transformation?\n",
    "* What does `filter()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pagecounts_en = pagecounts_tab.filter(lambda x: x[0] == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "**Write the code to cache the new RDD in the following cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[6] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_en.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now count the number of pages in English. The macro notebook `%time` will indicate how long it took Spark to count the number of entries in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 4 ms, total: 20 ms\n",
      "Wall time: 28.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4993702"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we told Spark to keep the dataset in memory, the time required to count the number of pages should be shorter for the second execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 4 ms, total: 8 ms\n",
      "Wall time: 3.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4993702"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pagecounts_en.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we committed an action on a cached RDD, it should now figure in the **Storage** section of our app's dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark app's dashboard link: http://localhost:4040/storage\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark app's dashboard link: {}/storage\".format(spark_helper.get_app_dashboard_url(sc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reduction Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in producing a bar chart of the number of pages request per language in our dataset. In order to do this, we will need to aggregate the number of requested pages for each language. This type of operation is called a *reduction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Transforming in Key-Value Pairs\n",
    "\n",
    "First, we transform our dataset to only keep the language and the number of requests. Furthermore, the number of request field is a string, we therefore use `int()` to convert it in an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecounts_tuple = pagecounts_tab.map(lambda entry: (entry[0], int(entry[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "**Take a look at the first 5 elements of the new RDD to confirm the transformation is correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aa.b', 3), ('aa.b', 1), ('aa.b', 1), ('aa.b', 1), ('aa.b', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_tuple.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark provides functions to work with key-value pairs. In our new dataset, the key is the language and the number of requests the value. The `key()` method of RDD returns a new RDD composed only of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa.b', 'aa.b', 'aa.b', 'aa.b', 'aa.b']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_tuple.keys().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `values()` method is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecounts_tuple.values().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Filtering Unrelated Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed, the key of our RDD sometimes has the form \"xx.x\". The project suffix \".x\" corresponds to the project name. The following abbreviations are used:  \n",
    "```\n",
    "wikibooks: \".b\"\n",
    "wiktionary: \".d\"\n",
    "wikimedia: \".m\"\n",
    "wikipedia mobile: \".mw\"\n",
    "wikinews: \".n\"\n",
    "wikiquote: \".q\"\n",
    "wikisource: \".s\"\n",
    "wikiversity: \".v\"\n",
    "mediawiki: \".w\"\n",
    "```\n",
    "Projects without a period and a following character are wikipedia projects. \n",
    "\n",
    "#### Exercise 3\n",
    "**Design a filter to only keep the pages that are Wikipedia projects.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagecounts_filt = pagecounts_tuple.filter(lambda x: len(x[0]) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Aggregating the Results by Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the number of pages equested for each language. In order to do this, we use the `reduceByKey()` method. As its name states, this function expect the RDD to be composed of key-value pairs.\n",
    "\n",
    "When called on a dataset of $(K, V)$ pairs, `reduceByKey()` returns a dataset of $(K, V)$ pairs where the values for each key are aggregated using the given reduce function *func*, which must be of type $(V,V) \\rightarrow V$.\n",
    "\n",
    "Since we want the total number of pages er language, our aggregating function will be the addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_pagecounts = pagecounts_filt.reduceByKey(lambda x, y: x + y).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reduceByKey()` is a transformation, therefore the result is a new RDD. To visualize the entire content of the latter, we can use the `collect()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sc', 818),\n",
       " ('ay', 321),\n",
       " ('uz', 3457),\n",
       " ('th', 35368),\n",
       " ('ja', 2953787),\n",
       " ('lg', 66),\n",
       " ('fo', 2873),\n",
       " ('tl', 5440),\n",
       " ('sh', 4809),\n",
       " ('sv', 93380),\n",
       " ('ak', 62),\n",
       " ('nb', 108),\n",
       " ('st', 100),\n",
       " ('it', 510621),\n",
       " ('rw', 72),\n",
       " ('ss', 133),\n",
       " ('su', 2362),\n",
       " ('sn', 42),\n",
       " ('ps', 1487),\n",
       " ('cs', 44709),\n",
       " ('ny', 64),\n",
       " ('co', 995),\n",
       " ('id', 62539),\n",
       " ('az', 3282),\n",
       " ('as', 507),\n",
       " ('ik', 68),\n",
       " ('mn', 1885),\n",
       " ('jp', 113),\n",
       " ('sm', 167),\n",
       " ('os', 2256),\n",
       " ('zu', 161),\n",
       " ('ts', 157),\n",
       " ('da', 22312),\n",
       " ('wo', 1615),\n",
       " ('ka', 6655),\n",
       " ('ne', 601),\n",
       " ('gu', 1771),\n",
       " ('ig', 309),\n",
       " ('sl', 21624),\n",
       " ('de', 1047880),\n",
       " ('sq', 6142),\n",
       " ('dz', 110),\n",
       " ('ff', 419),\n",
       " ('eu', 9447),\n",
       " ('tt', 1822),\n",
       " ('ch', 522),\n",
       " ('rm', 816),\n",
       " ('gl', 10335),\n",
       " ('yi', 3734),\n",
       " ('ug', 622),\n",
       " ('kn', 2429),\n",
       " ('ce', 1118),\n",
       " ('cu', 910),\n",
       " ('ku', 4204),\n",
       " ('ky', 348),\n",
       " ('kw', 866),\n",
       " ('hr', 23042),\n",
       " ('ng', 42),\n",
       " ('ti', 59),\n",
       " ('ty', 1040),\n",
       " ('lt', 29282),\n",
       " ('ln', 1639),\n",
       " ('es', 2520839),\n",
       " ('no', 45462),\n",
       " ('kr', 110),\n",
       " ('tw', 48),\n",
       " ('fy', 2224),\n",
       " ('rn', 38),\n",
       " ('ii', 292),\n",
       " ('sw', 2916),\n",
       " ('ve', 67),\n",
       " ('hz', 29),\n",
       " ('ga', 2491),\n",
       " ('ee', 289),\n",
       " ('sg', 128),\n",
       " ('lb', 3646),\n",
       " ('lv', 5806),\n",
       " ('gv', 1744),\n",
       " ('tk', 1012),\n",
       " ('ie', 373),\n",
       " ('bs', 12724),\n",
       " ('vi', 32105),\n",
       " ('tn', 96),\n",
       " ('li', 1795),\n",
       " ('so', 375),\n",
       " ('gn', 863),\n",
       " ('eo', 24401),\n",
       " ('cz', 9),\n",
       " ('bi', 102),\n",
       " ('cy', 4707),\n",
       " ('bh', 1767),\n",
       " ('la', 6597),\n",
       " ('ha', 128),\n",
       " ('cv', 2789),\n",
       " ('mh', 39),\n",
       " ('ab', 369),\n",
       " ('or', 113),\n",
       " ('ks', 104),\n",
       " ('af', 4385),\n",
       " ('mi', 899),\n",
       " ('is', 6336),\n",
       " ('mk', 6421),\n",
       " ('mr', 3733),\n",
       " ('zh', 207981),\n",
       " ('kv', 462),\n",
       " ('nv', 297),\n",
       " ('yo', 984),\n",
       " ('cr', 100),\n",
       " ('ki', 29),\n",
       " ('km', 1640),\n",
       " ('ro', 34017),\n",
       " ('el', 20445),\n",
       " ('pi', 299),\n",
       " ('na', 238),\n",
       " ('si', 897),\n",
       " ('fi', 56079),\n",
       " ('ml', 2922),\n",
       " ('oc', 3221),\n",
       " ('vo', 3380),\n",
       " ('pt', 950423),\n",
       " ('dk', 74),\n",
       " ('mt', 2223),\n",
       " ('jv', 2690),\n",
       " ('kk', 3251),\n",
       " ('ar', 89220),\n",
       " ('be', 3337),\n",
       " ('to', 374),\n",
       " ('br', 4918),\n",
       " ('bg', 14064),\n",
       " ('sr', 15010),\n",
       " ('mo', 142),\n",
       " ('en', 25576174),\n",
       " ('ur', 2422),\n",
       " ('te', 3363),\n",
       " ('he', 41558),\n",
       " ('ba', 1125),\n",
       " ('my', 468),\n",
       " ('uk', 30356),\n",
       " ('ta', 4113),\n",
       " ('ko', 82870),\n",
       " ('tg', 1833),\n",
       " ('pl', 546610),\n",
       " ('sa', 2378),\n",
       " ('hy', 2491),\n",
       " ('ms', 13026),\n",
       " ('om', 170),\n",
       " ('ho', 31),\n",
       " ('et', 20726),\n",
       " ('pa', 657),\n",
       " ('hu', 40725),\n",
       " ('an', 2833),\n",
       " ('qu', 2216),\n",
       " ('ru', 262440),\n",
       " ('za', 948),\n",
       " ('kg', 207),\n",
       " ('nn', 8880),\n",
       " ('fr', 938761),\n",
       " ('wa', 2289),\n",
       " ('tr', 78905),\n",
       " ('kl', 192),\n",
       " ('ca', 21381),\n",
       " ('fa', 17991),\n",
       " ('sd', 197),\n",
       " ('ht', 2163),\n",
       " ('se', 905),\n",
       " ('bm', 620),\n",
       " ('fj', 80),\n",
       " ('aa', 23),\n",
       " ('bn', 4735),\n",
       " ('bo', 217),\n",
       " ('iu', 229),\n",
       " ('lo', 1427),\n",
       " ('dv', 1640),\n",
       " ('xh', 98),\n",
       " ('hi', 6637),\n",
       " ('mg', 1614),\n",
       " ('nl', 184359),\n",
       " ('sk', 17093),\n",
       " ('kj', 21),\n",
       " ('gd', 1660),\n",
       " ('am', 1966),\n",
       " ('ia', 2179),\n",
       " ('av', 203),\n",
       " ('io', 3355)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_pagecounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Computing the Most Frequent Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to determine the 5 most frequent languages requested during the period observed. In order to do this, multiple solutions are available to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.1. Sort locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 25576174), ('ja', 2953787), ('es', 2520839), ('de', 1047880), ('pt', 950423)]\n"
     ]
    }
   ],
   "source": [
    "top5 = sorted(lang_pagecounts.collect(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.2 Ask Spark to sort the dataset using the `sortBy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 25576174), ('ja', 2953787), ('es', 2520839), ('de', 1047880), ('pt', 950423)]\n"
     ]
    }
   ],
   "source": [
    "top5 = lang_pagecounts.sortBy(keyfunc=lambda x: x[1], ascending=False).take(5)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4.3 Ask Spark for the top 5 using the `top()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 25576174), ('ja', 2953787), ('es', 2520839), ('de', 1047880), ('pt', 950423)]\n"
     ]
    }
   ],
   "source": [
    "top5 = lang_pagecounts.top(5, lambda x: x[1])\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "**Can you think of another method to get the same result?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 25576174), ('ja', 2953787), ('es', 2520839), ('de', 1047880), ('pt', 950423)]\n"
     ]
    }
   ],
   "source": [
    "top5 = lang_pagecounts.takeOrdered(5, lambda x: -x[1])\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Bar Chart\n",
    "\n",
    "Since we are in an interactive notebook, we can use Python plotting library `matplotlib` to plot our bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f34947d59e8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEVCAYAAADn6Y5lAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0k9Y/BLUs/8fxGaE3CVs/2bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34a0199908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "top5_t = list(zip(*top5))\n",
    "ax.bar(range(len(top5_t[0])), top5_t[1], width=0.35, align=\"center\")\n",
    "ax.set_xticks(range(len(top5_t[0])))\n",
    "ax.set_xticklabels(top5_t[0])\n",
    "ax.set_xlabel(\"lang\")\n",
    "ax.set_ylabel(u\"number of requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ending the Analysis\n",
    "\n",
    "Once the analysis is done, we need to tell Spark to free the resources and destroy the context using the `SparkContext`'s method `stop()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recap\n",
    "\n",
    "In this notebook, we used and learned about the following parts of \n",
    "**[Python Spark API](http://spark.apache.org/docs/latest/api/python/)**:\n",
    "1. Import Spark Python module: \n",
    "**[`import pyspark`](http://spark.apache.org/docs/latest/api/python/pyspark.html)**\n",
    "2. Create a SparkContext:\n",
    "**[`pyspark.SparkContext()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext)**\n",
    "2. Create an RDD from text files:\n",
    "**[`SparkContext.textFile(path)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile)**\n",
    "3. Take a the first *num* elements from an RDD: \n",
    "**[`Rdd.take(num)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take)**\n",
    "3. Count the number of elements in an RDD: \n",
    "**[`Rdd.count()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count)**\n",
    "4. Retrieve the first element of an RDD: \n",
    "**[`RDD.first()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.first)**\n",
    "5. Apply a transformation on each element of an RDD:\n",
    "**[`RDD.map(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map)**\n",
    "4. Cache an RDD:\n",
    "**[`RDD.cache()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cache)**\n",
    "5. Remove an RDD from memory: \n",
    "**[`RDD.unpersist()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.unpersist)**\n",
    "5. Filter an RDD:\n",
    "**[`RDD.filter(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.filter)**\n",
    "6. Merge the values for each keys: \n",
    "**[`RDD.reduceByKey(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey)**\n",
    "7. Get all elements of an RDD: \n",
    "**[`RDD.collect()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect)**\n",
    "8. Sort the elements of an RDD:\n",
    "**[`RDD.sortBy(func)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.sortBy)**\n",
    "9. Get the top $N$ elements from an RDD:\n",
    "**[`RDD.top(N)`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.top)**\n",
    "10. End the SparkContext:\n",
    "**[`SparkContext.stop()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.stop)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 11. References\n",
    "\n",
    "* [Berkeley AmpCamp 5 - Data Exploration Using Spark](http://ampcamp.berkeley.edu/5/exercises/data-exploration-using-spark.html)\n",
    "* [edX - Introduction to Big Data with Apache Spark](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x)\n",
    "* [edX - Introduction to Big Data with Apache Spark (Github repo)](https://github.com/spark-mooc/mooc-setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
